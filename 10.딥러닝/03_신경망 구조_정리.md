# DNN(Deep Neural Network)
- 신경망 구성요소
  - 층/손실함수/optimizer
- 한 노드의 구성
  - 입력값 / 가중치 / 편향 / 활성함수
- 층(Layer)
  - 입력층(Input Layer): 입력값을 받아 Hidden Layer에 전달하는 노드들로 구성된 층
  - 출력층(Output Layer): 예측결과를 출력하는 노드로 구성된 층
  - 은닉층(Hidden Layer)
- 모델(Network/Model)
  - Layer를 쌓아 구성. 적절한 모델을 찾는 것은 과학적이기 보단 경험에 의한 방식, 그리고 기존의 좋은 성능을 가진 구조를 기반으로 접근

### 딥러닝
- 신경망이 많아지면 딥러닝으로 호칭. 과거엔 은닉층이 3개 정도만 되도 딥러닝이라고 호칭하였으나 현재는 아니다.


### 손실함수
- 모델을 통해 얻은 예측값과 실제 데이터의 차이를 계측하는 함수
- 해결하려는 문제의 종류에 따라 표준적인 손심함수 존재
1. 이진분류 - `binary_crossentropy`를 손실함수로 사용<br/>
  ![2021-04-19 17;53;36](https://user-images.githubusercontent.com/71580318/115209091-3ab34600-a138-11eb-88b1-304609b622ad.PNG)
2. 다중 클래스 분류 - `categorical_crossentropy`를 손실함수로 사용<br/>
  ![2](https://user-images.githubusercontent.com/71580318/115209144-4868cb80-a138-11eb-956f-082b65494b85.PNG)
3. 회귀 - `Mean squared error`를 손실함수로 사용<br/>
  ![3](https://user-images.githubusercontent.com/71580318/115209172-50c10680-a138-11eb-864e-b158d537a306.PNG)

- 평가지표
  - 모델의 성능을 평가 <-> 손실함수: 모델 학습 중도에 가중치 업데이트를 위한 오차 구하기 위한 용도

### 활성함수
- 각 유닛이 입력결과를 처리한 후 출력 전 거치는 함수
1. Sigmoid(S자형 모양의)(Logistic)<br/>
![11](https://user-images.githubusercontent.com/71580318/115210071-320f3f80-a139-11eb-92ae-6da4a50e7634.PNG)
- 초기 딥러닝 모델 활성함수로 많이 사용, but 레이어가 깊을수록 기울기소실 문제 야기
- 함수값 중심이 0이 아니어 학습속도 저하

2. Hyperbolic tangent<br/>
![22](https://user-images.githubusercontent.com/71580318/115210323-70a4fa00-a139-11eb-8c65-dbdd73a9306c.PNG)
- sigmoid보단 학습효율 좋으나 여전히 기울기 소실 문제 존재

3. ReLU(Rectified Linear Unit)<br/>
![33](https://user-images.githubusercontent.com/71580318/115210458-8f0af580-a139-11eb-9735-773522e78634.PNG)
- 기울기 소실 문제 해결, but 0이하의 값을 받는 뉴런이 죽음

4. Leaky ReLU<br/>
![44](https://user-images.githubusercontent.com/71580318/115210526-a77b1000-a139-11eb-8f51-16bf561b8c84.PNG)
- ReLU의 노드가 죽는 현상 해결

5. Softmax<br/>
![55](https://user-images.githubusercontent.com/71580318/115210683-ced1dd00-a139-11eb-94dc-43c888bf7ec9.PNG)
- 다중 분류를 위한 네트워크 출력층 활성함수로 사용, 출력은 각 결과에 대한 확률로


### 최적화 방법
- 손실함수를 기반으로 네트워크 매개변수를 업데이트하는 알고리즘
1. 경사하강법<br/>
![66](https://user-images.githubusercontent.com/71580318/115211245-57507d80-a13a-11eb-9e88-c2bee4bddbf8.PNG)

2. 배치 경사하강법
- loss를 계산할 떄 전체 학습데이터를 사용해 그 평균값으로 파라미터 최적화할 값 계산 -> 많은 계산량 -> 메모리 부족 현상 발생 가능성

3. 미니배치 확률적 경사 하강법(Mini Batch Stochastic Gradient Decent)
- loss 계산시 지정한 batch size만큼 계산하여 파라미터 업데이트
- 계산 속도 빠르나 최적값을 찾아가는 방향이 불안정 -> 반복 횟수 늘려 batch 방식과 유사한 결과 획득 가능
- `step`: 배치 한번을 학습하여 한번 파라미터를 업데이트하는 단위

### 오차 역전파(Back Propagation)
- 손실값으로 파라미터 업데이트하는 과정
- 손실값의 미분값을 이용하며 Chain rule을 사용해 한단계씩 파라미터 업데이트
- SGD를 기반으로 한 주요 옵티마이저
  - 방향성을 개선한 최적화 방법
    - Momentum
    - NAG(Nesterov Accelerated Gradient)
  - 학습률을 개선한 최적화 방법
    - Adagrad
    - RMSProp
  - 방향성 + 학습률 개선 최적화 방법
    - Adam<br/>
![다운로드](https://user-images.githubusercontent.com/71580318/115212286-4e13e080-a13b-11eb-9472-7a11a5ebcb19.png)

이전 카카오 유명엔지니어 딥러닝 개념설명:
https://www.slideshare.net/yongho/ss-79607172
