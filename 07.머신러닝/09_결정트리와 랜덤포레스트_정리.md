# 결정트리와 렌덤포레스트
### 01. 의사결정나무(Decision Tree)
- 이전의 내용 학습
- - -
### 02. 앙상블(Ensemble) 기법
- `주의!` SVM모델은 결과를 확률로 확인하기 위해 매개변수 `probability=True` 추가하자.
- 이전까지 배워왔던 학습 모델을 여러 개 사용하여 성능을 높이는 깁버
- 개별 모델의 과적합 또한 예방 가능
- 종류
  1. 투표방식
    1) `Bagging`: 같은 유형의 모델을 조합 + 각각 다른 데이터 학습
    2) `Voting`: 다른 유형 모델 조합 + 같은 데이터 학습
  2. `Boosting(부스팅)`: 약한 학습기들 결합하여 강력한 학습모델 제작, 첫번째 이후의 학습기들은 앞의 학습기의 에러를 기준으로 학습

### 03. `Random Forest`
- Bagging 방식의 앙상블 모델, 결정트리 기반
- 빠른 처리속도, 성능 또한 좋음
- 하이퍼파라미터
  - 각 트리는 부트스트랩 샘플링(중복 허용, 랜덤한 샘플링 방식)으로 데이터셋 준비
  - 각 트리는 전체 피쳐 중 일부 피쳐만 랜덤하게 가짐
  - `n_estimators`: tree의 개수, 시간과 메모리가 허용하는 범위에서 클수록 좋다. 
  - `max_features`: 각 트리에서 선택할 feature의 개수, 클수록 각 트리간의 feature 차이가 감소
  - `max_depth`, `min_samples_leaf`, ..:
    - DecisionTreeClassifier의 하이퍼파라미터
    - 트리의 최대 깊이, 가지를 치기 위한 최소 샘플 수
```python
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

rf = RandomForestClassifier(n_estimators=200, # DicisionTree 개수
                           max_features=7, # 각각의 tree 학습시 전달할 feature(컬럼)의 개수
                           n_jobs=-1,
                           random_state=1)
rf.fit(X_train, y_train)
pred_train = rf.predict(X_train)
pred_test = rf.predict(X_test)
accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test)

fi = rf.feature_importances_
fi_s = pd.Series(fi, index=X_train.columns)
fi_s.sort_values(ascending=False)

# Feature 중요도
fi_s.sort_values().plot(kind='barh', figsize=(10,7))
plt.show()
```
