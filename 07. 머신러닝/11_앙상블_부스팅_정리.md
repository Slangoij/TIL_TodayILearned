# Boosting
### 01. GradientBoosting
- 개별 모델로 Decision Tree 를 사용한다. 
- depth가 깊지 않은 트리를 많이 연결해서 이전 트리의 오차를 보정해 나가는 방식으로 실행한다.
- 오차를 보정할 때 경사하강법(Gradient descent)을 사용한다.
- 얕은 트리를 많이 연결하여 각각의 트리가 데이터의 일부에 대해 예측을 잘 수행하도록 하고 그런 트리들이 모여 전체 성능을 높이는 것이 기본 아이디어.
- 분류와 회귀 둘다 지원하는 모델 (GradientBoostingClassification, GrandientBoostingRegressor)
- 훈련시간이 많이 걸리고, 트리기반 모델의 특성상 희소한 고차원 데이터에서는 성능이 않좋은 단점이 있다.
- 주요 파라미터
  - Decision Tree의 가지치기 관련 매개변수 ex) max_depth, max_nodes
  - `learning rate`: 이전 tree의 오차를 얼마나 강하게 보정할 것인지 제어하는 값. 
    - 값이 크면 보정을 강하게 하여 모델 복잡도 상승. 학습데이터의 정확도가 상승하지만 과대적합 가능성 존재
    - 값을 작게 잡으면 보정을 약하게 하여 모델 복잡도 감소. 과대적합을 줄일 수 있지만 정확도 감소 가능성 발생
    - 기본값 : 0.1
  - `n_estimators`: tree의 개수 지정. 많을수록 복잡한 모델
  - `n_iter_no_change`, `validation_fraction`: validation_fraction(비율)만큼 n_iter_no_change(반복 횟수)동안 검증점수가 개선되지 않으면 훈련을 조기 종료
  - 보통 max_depth를 낮춰 개별 트리의 복잡도를 낮춘다. (5가 넘지 않게) 그리고 n_estimators를 가용시간, 메모리 한도에 맞춘뒤 적절한 learning_rate을 찾는다.
```python
# 필요 라이브러리 임포트 후
gb = GradientBoostingClassifier(random_state=1)
gb.fit(X_train, y_train)
```
### 02. XGBoost(Extra Gradient Boost)
- Gradient Boost 알고리즘을 기반으로 개선해서 나온 모델.
- 캐글 경진대회에서 상위에 입상한 데이터 과학자들이 사용한 것으로 알려저 유명해짐.
- Gradient Boost의 단점인 느린수행시간을 해결하고 과적합을 제어할 수 있는 규제를 제공하여 성능을 높임.
- 두가지 개발 방법
  - [Scikit-learn 래퍼 XGBoost 모듈 사용](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)
  - [파이썬 래퍼 XGBoost 모듈 사용](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training)
- 설치   
``
pip install xgboost
conda install -y -c anaconda py-xgboost(cmd 창에서 질문이 나왔을 시 `-y`를 yes로 입력한 것으로 인식)
``

### 03. Scikit-learn 래퍼 XGBoost
- XGBoost를 Scikit-learn프레임워크와 연동할 수 있도록 개발한 래퍼
- Scikit-learn의 Estimator들과 동일한 패턴으로 코드를 작성 가능
- GridSearchCV나 Pipeline 등 Scikit-learn이 제공하는 다양한 유틸리티 사용 가능
- `XGBClassifier`: 분류 / `XGBRegressor` : 회귀 
- 주요 매개변수
  - learning_rate : 학습률, 보통 0.01 ~ 0.2 사이의 값 사용
  - n_estimators : week tree 개수
  - max_depth: 트리의 depth 지정.
```python
from xgboost import XGBClassifier

xgb = XGBClassifier(n_estimators=200,
                   learning_rate=0.5,
                   max_depth=2,
                   random_state=1)
```
