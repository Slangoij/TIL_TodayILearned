# 04. 모델 평가

### 분류와 회귀 모델의 평가방법
- 분류 평가 지표
  1. 정확도 (Accuracy)
  2. 정밀도 (Precision)
  3. 재현률/민감도 (Recall)
  4. F1점수 (F1 Score)
  5. PR Curve, AP
  6. ROC, AUC
- 회귀 평가 지표
  1. MSE (Mean Squared Error)
  2. RMSE (Root Mean Squared Error)
  3. $R^2$ (결정계수)
- sckit-learn 평가함수 
  - sklearn.metrics 모듈을 통해 제공


### 1. 분류 평가 방법
1) `정확도`
  - 용어: 양성 - 찾으려는 대상/ 음성 - 찾으려는 대상이 아닌 것
  - `정확도` = 맞게 예측한 건수 / 전체 예측 건수
    - 한계: 불균형 데이터의 경우 정확한 평가 불가
    - `MNIST Data set`으로 확인
  ```python
  import numpy as np
  import matplotlib as mpl
  import matplotlib.pyplot as plt
  from sklearn.datasets import load_digits
  
  digits = load_digits()
  X, y = digits['data'], digits['target']
  
  # 숫자 데이터셋 플롯하여 확인
  plt.figure(figsize=(10,10))
  for i in range(9):
      plt.subplot(3,3,i+1)
      plt.imshow(X[i].reshape(8,8), cmap='Greys') # 0 ~ 8
  #     plt.axis('off')
      plt.xticks([])
      plt.yticks([])
      plt.title(y[i], fontdict={'fontsize':25})
  plt.show()
  
  # 불균형 데이터셋으로 변형
  y = y == 9 # 9: True(1), 나머지: False(0)
  # 훈련, 테스트셋 분할
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)
  ```
2) `혼동 행렬(Confusion Matrix)`
- 분류의 평가지표의 기준으로 사용된다.
- 혼동행렬을 이용해 다양한 평가지표(정확도, 재현률, 정밀도, F1 점수, AUC 점수)를 계산할 수 있다.
- 함수: confusion_matrix(정답, 모델예측값)
- 결과의 0번축: 실제(Ground Truth) class, 1번 축: 예측 class
- 이진 분류 평가 점수 (`sklearn.metrics`모듈)
  ![다운로드](https://user-images.githubusercontent.com/71580318/112115692-d1a1e680-8bfc-11eb-8c76-e73c168e9f95.png)
  - 정확도(Accuracy) : 전체 데이터 중 맞게 예측한 건수 비율
    - `confusion_matrix(y 실제값, y 예측값)`
  - 재현율/민감도(Recall/Sensivity) : 실제 양성 건수 중 양성으로 예측한 건수 비율
    - `recall_score(y 실제값, y 예측값)`
  - 정밀도(Precision) : 양성으로 에측한 건수 중 실제 양성 비율
    - `precision_score(y 실제값, y 예측값)`
  - F1 점수(F1-score) : 재현율과 정밀도의 조화평균 점수. 즉, 한쪽으로 치우치지 않은 정도
    - `f1_score(y 실제값, y 예측값)`
  - 종합점수지표
    - `classification_report(y 실제값, y 예측값)`
  - 기타
    - 특이도(Specificity) : 실제 음성 건수 중 음성으로 예측한 건수 비율
    - 위양성률(Fall out) : 실제 음성 건수 중 양성으로 잘못 예측한 건수 비율
- 머신러닝 모델 이용한 학습 예제
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# 모델 생성
tree = DecisionTreeClassifier(max_depth=3)
rf = RandomForestClassifier(n_estimators=200) #, max_depth=2)

# 모델 학습
tree.fit(X_train, y_train)
rf.fit(X_train, y_train)

# 예측 및 평가
pred_train_tree = tree.predict(X_train)
pred_test_tree = tree.predict(X_test)

pred_train_rf = rf.predict(X_train)
pred_test_rf = rf.predict(X_test)

# classification_report()로 전반적 평가지표 
from sklearn.metrics import classification_report
result = classification_report(y_test, pred_test_rf, target_names=['pos(not 9)', 'pos(9)'])
print(result) # False가 양성일 때 점수, True가 양성일 때 점수를 구분하여 표기
```

3) `PR Curve(Precision Recall Curve), AP Score(Average Precision Score)`
- `PR Curve`: `Threshold(임계값)`을 0에서 1까지 늘려가면서 recall(감소)과 Precision(증가)의 변화를 Curve로 플롯
- `AP Score`: 플롯된 그래프 아래 면적(0 ~ 1 사이의 값)
```python
from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, average_precision_score

pos_proba = tree.predict_proba(X_test)[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_test, pos_proba) # y, pos_예측확률

# Curve그리기
plt.figure(figsize=(7,7))
plt.plot(recalls, precisions, marker='o') # X: recall, Y: precision
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.grid(True)
plt.show()

# 실제 값에 반영되는 그래프
fig, ax = plt.subplots(1,1,figsize=(7,7))
# plt.gca()
plot_precision_recall_curve(tree, # 모델
                           X_test, # X값
                           y_test, # y값
                           ax=ax)
plt.show()

# ap score
average_precision_score(y_test, pos_proba) # AP score(y, pos_예측확률)
```

3) `ROC Curve(Receiver Operating Characteristic), AUC(Area Under the Curve) Score`
- `ROC Curve`: `Threshold(임계값)`을 0에서 1까지 늘려가면서 FPR(False Positive Rate/위양성률)(감소)과 TPR(True Positive Rate/재현율)(감소) 플롯
- `AUC Score`: 플롯된 그래프 아래 면적(0 ~ 1 사이의 값)
- ROC vs. PR 용도: PR은 양성 클래스 탐지가 음성클래스보다 중요할 때(ex)암환자 진단, ROC는 중요도 비슷할 때
```python
from sklearn.metrics import roc_curve, plot_roc_curve, roc_auc_score

# 결정트리와 RandomForest 동시 
pos_proba_tree = tree.predict_proba(X_test)[:, 1]
pos_proba_rf = rf.predict_proba(X_test)[:, 1]

fpr_tree, tpr_tree, threshold_tree = roc_curve(y_test, pos_proba_tree) # y, pos_예측확률
fpr_rf, tpr_rf, threshold_rf = roc_curve(y_test, pos_proba_rf)
fpr_rf.shape, tpr_rf.shape, threshold_rf.shape

plt.figure(figsize=(8,8))
ax = plt.gca()
plot_roc_curve(tree, X_test, y_test, ax=ax)
plot_roc_curve(rf, X_test, y_test, ax=ax)
plt.show()

print("tree:", (roc_auc_score(y_test, pos_proba_tree)))
print("random forest:", (roc_auc_score(y_test, pos_proba_rf)))
```
