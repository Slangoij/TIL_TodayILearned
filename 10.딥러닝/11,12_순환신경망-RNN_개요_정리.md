# 순환신경망(RNN)(Recurrent Neural Network)
- 순서가 중요한( ex)시계열(기온변화, 주가) ) 순차데이터를 토대로 예측을 하기 위한 모델
- 순차데이터 예시
  1. sequence-to-vector(many to one): 순서에 따른 여러 값이 들어가면 하나의 값(vector)이 출력, 예시-주가예측
  2. sequence-to-sequence(many to many): 순차데이터 입력, 순차데이터 출력, 예시-번역
  3. vector-to-sequence(one to many): 하나의 값이 입력, 순차데이터 출력, 예시-Image captioning(이미지설명 문장 만들기)

## RNN 개요
- 기억시스템(Memory System): 순차가 중요하므로 입력데이터의 순서를 기억하여야 한다.
=> FC나 Conv층은 이러한 순서 반영 불가능 => 새로운 모델 필요성

## 01. Simple RNN
- 각 입력데이터 순차 입력, 또한 이전 입력에 대한 출력 데이터 동시에 입력하여 함께 연산
- ![다운로드](https://user-images.githubusercontent.com/71580318/116980379-20b16000-ad01-11eb-8492-fc84598f5e70.png)
- 한계: sequence가 긴 경우 앞쪽의 기억이 소실 => 개선: LSTM, GRU

## 02. LSTM(Long Short Term Memory)
- RNN을 개선하여 장기기억 데이터 또한 반영
- 구조:
- ![다운로드 (1)](https://user-images.githubusercontent.com/71580318/116980791-a1705c00-ad01-11eb-96b3-0f1899e43fcb.png)
  - Forget gate: 현재 노드의 입력값을 기준으로 cell state(장기 기억)의 값에서 얼마나 잊을 지 결정
  - Input gate: 현재 노드의 입력값을 cell state에 추가
  - Output gate: 다음 노드의 입력값(hidden state)으로 전달
- Model 생성
  - keras로 생성시 매개변수: return_sequences=True 시, 시퀀스마다 결과 출력 => 다음 lstm 레이어로 결과 전달시 or many-to-many 문제 결과로 전달시
```python
# df를 순차데이터로 가정
# 중간중간 하이퍼파라미터나 데이터셋은 생략

# X,y 분리
df_y = df['Close'].to_frame() # df['Close']: Series    Series.to_frame(): Series=>DataFrame (2차원)

# scaling
from sklearn.preprocessing import MinMaxScaler
X_scaler = MinMaxScaler()
y_scaler = MinMaxScaler()
X = X_scaler.fit_transform(df_X)
y = y_scaler.fit_transform(df_y)

# 기준 window_size만큼 데이터 분할
window_size = 50 # 한개 input값에 들어갈 연속된 값의 개수
data_X = [] # [[1-50], [2-51], [3-52], .....]
data_y = [] # [51,     52,      53,  , .....]

for i in range(len(y)-window_size):
    data_X.append(X[i:i+window_size])
    data_y.append(y[i+window_size])
    
# train:test = 8:2 비율로 분리
train_index = int(len(data_y)*0.8)
X_train, y_train = np.array(data_X[:train_index]), np.array(data_y[:train_index])
X_test, y_test = np.array(data_X[train_index:]), np.array(data_y[train_index:])

# Dataset 구성
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(N_TRAIN).batch(N_BATCHS, drop_remainder=True).repeat()
test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(N_BATCHS)

# Model 생성
# 모델 구성
def create_model():
    model = keras.Sequential()
    model.add(layers.Input((window_size,5)))
    # LSTM
    model.add(layers.LSTM(32, activation='relu', return_sequences=False))
    model.add(layers.Dense(32, activation='relu'))
    model.add(layers.Dense(1)) # 회귀
    
    return model
    
model = create_model()
model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
             loss='mse')
model.summary()
```
